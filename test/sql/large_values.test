# name: test/sql/large_values.test
# description: Tests with extreme and boundary decimal values
# group: [thdck_spark_funcs]

require thdck_spark_funcs

# --- Tests using DECIMAL(10,0) max value (10 nines) ---
# DECIMAL(10,0) / DECIMAL(10,0): result type DECIMAL(21,11), scale_adj = 11

# Max DECIMAL(10,0) value divided by 1
query I
SELECT spark_decimal_div(9999999999::DECIMAL(10,0), 1::DECIMAL(10,0));
----
9999999999.00000000000

# Division of a value by itself = 1.000...
# 9999999999 / 9999999999 = 1.00000000000
query I
SELECT spark_decimal_div(9999999999::DECIMAL(10,0), 9999999999::DECIMAL(10,0));
----
1.00000000000

# Max value / -1 (negation)
query I
SELECT spark_decimal_div(9999999999::DECIMAL(10,0), -1::DECIMAL(10,0));
----
-9999999999.00000000000

# Negative max / positive
query I
SELECT spark_decimal_div(-9999999999::DECIMAL(10,0), 1::DECIMAL(10,0));
----
-9999999999.00000000000

# 1 / max (very small result near zero)
# 1 / 9999999999 = 0.0000000001000... (approximately 10^-10)
# scale_adj = 11, so: 1 * 10^11 / 9999999999 = 100000000000 / 9999999999
# = 10.00000001000... quotient = 10, remainder = 10
# 2*10 = 20 >= 9999999999? No -> no round
# Result: 0.00000000010
query I
SELECT spark_decimal_div(1::DECIMAL(10,0), 9999999999::DECIMAL(10,0));
----
0.00000000010

# Values near zero
# DECIMAL(10,6): 0.000001 / 1.000000
# DECIMAL(10,6) / DECIMAL(10,6): result_scale=max(6, 6+10+1)=17
# prec = (10-6)+6+17 = 27, so DECIMAL(27,17)
# scale_adj = 17 - 6 + 6 = 17
# a=1 (0.000001 stored as 1), b=1000000 (1.000000 stored as 1000000)
# 1 * 10^17 / 1000000 = 100000000000 -> 0.00000100000000000
query I
SELECT spark_decimal_div(0.000001::DECIMAL(10,6), 1.000000::DECIMAL(10,6));
----
0.00000100000000000

# Verify result type
query I
SELECT typeof(spark_decimal_div(0.000001::DECIMAL(10,6), 1.000000::DECIMAL(10,6)));
----
DECIMAL(27,17)

# Small value / small value = 1
# 0.000001 / 0.000001 = 1.00000000000000000
query I
SELECT spark_decimal_div(0.000001::DECIMAL(10,6), 0.000001::DECIMAL(10,6));
----
1.00000000000000000

# Large precision numerator with high scale
# DECIMAL(38,18) / DECIMAL(10,2)
# result_scale = max(6, 18+10+1) = 29
# prec = (38-18)+2+29 = 51 -> capped to 38
# overflow adj: int_digits=51-29=22, min_scale=min(29,6)=6, 38-22=16 >= 6 -> adjusted_scale=16
# result type: DECIMAL(38,16)
query I
SELECT typeof(spark_decimal_div(1.000000000000000000::DECIMAL(38,18), 1.00::DECIMAL(10,2)));
----
DECIMAL(38,16)

# DECIMAL(38,18) / DECIMAL(10,2) value check
# scale_adj = 16 - 18 + 2 = 0
# a = 1000000000000000000 (1.0 stored), b = 100 (1.00 stored)
# scaled_a = a * 10^0 = a
# 1000000000000000000 / 100 = 10000000000000000
# -> 0.0010000000000000000 ... wait, result is DECIMAL(38,16)
# 10000000000000000 as DECIMAL(38,16) = 1.0000000000000000
query I
SELECT spark_decimal_div(1.000000000000000000::DECIMAL(38,18), 1.00::DECIMAL(10,2));
----
1.0000000000000000

# Asymmetric precision: large precision numerator, small denominator
# DECIMAL(38,18) / DECIMAL(3,0)
# result_scale = max(6, 18+3+1) = 22
# prec = (38-18)+0+22 = 42 -> capped to 38
# overflow adj: int_digits=42-22=20, min_scale=min(22,6)=6, 38-20=18 >= 6 -> adjusted_scale=18
# result type: DECIMAL(38,18)
query I
SELECT typeof(spark_decimal_div(1.000000000000000000::DECIMAL(38,18), 1::DECIMAL(3,0)));
----
DECIMAL(38,18)

# scale_adj = 18 - 18 + 0 = 0
# a = 1000000000000000000, b = 1
# result = 1000000000000000000 -> 1.000000000000000000
query I
SELECT spark_decimal_div(1.000000000000000000::DECIMAL(38,18), 1::DECIMAL(3,0));
----
1.000000000000000000

# Two divided by three with high-scale types
# DECIMAL(38,18) / DECIMAL(38,18)
# result type: DECIMAL(38,6) (from precision_rules.test)
# scale_adj = 6 - 18 + 18 = 6
# a = 2 * 10^18, b = 3 * 10^18
# scaled_a = 2 * 10^18 * 10^6 = 2 * 10^24
# quotient = 2 * 10^24 / (3 * 10^18) = 2000000/3 = 666666 remainder 2
# 2*2 = 4 >= 3 -> round up -> 666667
# Result: 0.666667
query I
SELECT spark_decimal_div(2.000000000000000000::DECIMAL(38,18), 3.000000000000000000::DECIMAL(38,18));
----
0.666667

# Division by -1 with high scale
query I
SELECT spark_decimal_div(2.000000000000000000::DECIMAL(38,18), -1.000000000000000000::DECIMAL(38,18));
----
-2.000000

# DECIMAL(18,0) values: large integer division
# 999999999999999999 / 7
# DECIMAL(18,0) / DECIMAL(18,0): result_scale=max(6, 0+18+1)=19
# prec=(18-0)+0+19=37, so DECIMAL(37,19)
# scale_adj = 19 - 0 + 0 = 19
# 999999999999999999 * 10^19 / 7
# = 142857142857142857 * 10^19 + remainder
# 999999999999999999 / 7 = 142857142857142857.0000000000000000000 (+ rounding)
# 999999999999999999 = 7 * 142857142857142857 + 0 (since 7 * 142857142857142857 = 999999999999999999)
# So it's exact!
query I
SELECT spark_decimal_div(999999999999999999::DECIMAL(18,0), 7::DECIMAL(18,0));
----
142857142857142857.0000000000000000000

# Very small numerator, large denominator
# 1 / 999999999999999999
# scale_adj = 19
# 1 * 10^19 / 999999999999999999 = 10000000000000000000 / 999999999999999999
# = 10 remainder 10
# 2*10 = 20 >= 999999999999999999? No -> no round
# Result: 0.0000000000000000010 (10 in last 19 decimal places)
query I
SELECT spark_decimal_div(1::DECIMAL(18,0), 999999999999999999::DECIMAL(18,0));
----
0.0000000000000000010

# Verify result type for DECIMAL(18,0) / DECIMAL(18,0)
query I
SELECT typeof(spark_decimal_div(1::DECIMAL(18,0), 1::DECIMAL(18,0)));
----
DECIMAL(37,19)
